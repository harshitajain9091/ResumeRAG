ResumeRAG — Overview

Goal: Accept multiple resumés, parse & embed their content, support QA with snippet evidence (/api/ask) and match candidates to jobs with evidence & missing requirements (POST /api/jobs/:id/match). Judge requirements (pagination, idempotency, rate-limiting, PII redaction, ZIP bulk) are addressed.

README (ready-to-copy)
ResumeRAG

API server for uploading resumés, searching candidates and matching jobs.

Quick start (assumptions)

Backend: FastAPI + SQLite (or Postgres) + FAISS for vector store.

Embeddings: any deterministic embedding model (use fixed model/version).

Run uvicorn main:app --reload for dev.

API Summary (required endpoints)

All endpoints under /api.

Authentication

POST /api/register
Body: { "email", "password", "role": "recruiter" | "user" }
Response: { "token" }

POST /api/login
Body: { "email", "password" } → { "token" }
Use Authorization: Bearer <token> for protected endpoints. Recruiter role sees redacted PII differently.

Resumés

POST /api/resumes (multipart upload)
Headers: Idempotency-Key: <key>
Body: multipart: file (pdf/docx/txt) or zip (bulk)
Response: 201 { "id", "status": "processed" } or 202 processing

GET /api/resumes?limit=&offset=&q=
Response: { "items": [...], "next_offset": number | null }

GET /api/resumes/:id
Response: { "id","name","redacted":bool,"parsed_text","skills":[...], "created_at" }

Ask (RAG QA)

POST /api/ask
Body: { "query": "text", "k": 5 }
Response:

{
  "answer": "string",
  "sources": [
    {"resume_id": "uuid","snippet": "text snippet", "start": 123, "end": 234, "score": 0.912}
  ]
}

Jobs

POST /api/jobs
Body: { "title", "description", "requirements": ["r1","r2"], "preferred": ["p1"] }
Headers: Idempotency-Key
Response: { "id", "title", "created_at" }

GET /api/jobs/:id
Response: { "id","title","description","requirements","preferred" }

POST /api/jobs/:id/match
Body: { "top_n": 10 }
Response:

{
  "matches": [
    {
      "resume_id": "uuid",
      "score": 0.87,
      "evidence": [
        {"snippet":"text","requirement_matched":"Docker","start":10,"end":50}
      ],
      "missing_requirements": ["Kubernetes"]
    }
  ]
}

Pagination

When returning lists:

Accept limit (default 20, max 100) and offset (default 0).

Response includes next_offset (null if none).

Idempotency

All POSTs that create resources MUST accept Idempotency-Key. Replay of same key within a TTL returns same resource and does not create duplicates.

Rate Limiting

Enforce 60 req/min/user (IP for unauth). On exceed respond:

HTTP 429
{ "error": { "code":"RATE_LIMIT" } }

Error format

Always:

{ "error": { "code": "FIELD_REQUIRED", "field": "email", "message": "Email is required" } }

Seed data & test credentials

Test user (recruiter): email: judge@resumerag.test, password: Test1234!, role: recruiter.

Test user (regular): email: user@resumerag.test, password: Test1234!, role: user.

Seed jobs: three sample jobs (Frontend Engineer, Data Scientist, DevOps).

Seed 5 resumés (see sample below).

Example requests & responses (curl)

Upload single resume

curl -X POST "http://localhost:8000/api/resumes" \
  -H "Authorization: Bearer <token>" \
  -H "Idempotency-Key: upload-1" \
  -F "file=@/path/John-Doe-Resume.pdf"


Response:

201
{
  "id": "r_123",
  "status": "processed"
}


Search resumes

curl "http://localhost:8000/api/resumes?limit=5&offset=0&q=python"


Response:

{
  "items": [
    {"id":"r_123","name":"John Doe","snippet":"Python backend dev...","score":0.91}
  ],
  "next_offset": 5
}


Ask

curl -X POST "http://localhost:8000/api/ask" -H "Authorization: Bearer <token>" \
  -d '{"query":"experience with kafka and spark", "k": 3 }'


Response:

{
  "answer":"Two candidates mention Kafka; one shows Spark in multiple projects.",
  "sources":[
    {"resume_id":"r_123","snippet":"Built real-time pipelines using Kafka and Spark streaming", "start":120, "end":170, "score":0.93}
  ]
}


Match job

curl -X POST "http://localhost:8000/api/jobs/ job_1/match" -H "Authorization: Bearer <token>" \
  -d '{"top_n":5}'


Response:

{
  "matches":[
    {
      "resume_id":"r_123",
      "score":0.92,
      "evidence":[{"snippet":"5 years with Docker and Kubernetes","requirement_matched":"Kubernetes","start":22,"end":60}],
      "missing_requirements":["GCP"]
    }
  ]
}

Data model (suggested SQL/ORM)

users

id (uuid PK), email, password_hash, role (recruiter|user), created_at

resumes

id (uuid), user_id, filename, original_name, parsed_text (TEXT), skills (JSON), redacted_text (TEXT), created_at, processed_at, pii_redacted boolean

jobs

id, user_id, title, description, requirements (JSON array), preferred (JSON array), created_at

embeddings

id (uuid), resume_id, chunk_id, vector (float array or stored in FAISS), text_snippet, start_idx, end_idx, created_at

idempotency_keys

key, user_id, endpoint, resource_id, created_at, response_snapshot

rate_limits (in-memory store like Redis recommended)

track requests per minute per user

Processing pipeline (upload → searchable vectors)

Upload (single file or ZIP). If ZIP, iterate files and enqueue each file as separate resume.

Parse: Extract text (pdf → text via pdfminer/pdfplumber, docx via python-docx), normalize whitespace.

Chunk: Split text into overlapping chunks (e.g., 400 tokens with 50-token overlap). Save chunk start/end char offsets.

Redact PII: Using deterministic regex + heuristics (emails, phone, SSNs, addresses). Store both redacted_text (visible to non-recruiter) and parsed_text (only recruiter sees PII). Redaction replaces with [REDACTED_EMAIL] etc.

Embed: Use deterministic embedding model/version; store vectors in FAISS (or SQLite + vector extension).

Index metadata: Save skill extraction (simple NLP rules + regex for years of experience).

Processing status: Return processed when finished. Judge requires 3+ uploads processed.

Deterministic ranking

Use cosine similarity on vectors; ensure embedding model and pre/post-processing deterministic (fixed tokenizer, lowercase rules).

When scores tie, deterministic tiebreaker: hash(resume_id) (use SHA256; pick higher/consistent order). This ensures reproducible ordering.

PII Redaction rules

Apply for all users: remove emails, phone numbers, national IDs, physical addresses, dates-of-birth.

Recruiter role: can request unredacted via ?reveal_pii=true (only if role= recruiter). Default API never returns unredacted text to non-recruiter.

Evidence snippets returned via /api/ask and /api/jobs/:id/match must be redacted unless recruiter.

ZIP bulk upload

POST /api/resumes accepts file content-type application/zip. Server will extract and process each file as separate resume. Respect idempotency key for the zip as a batch.

Rate limiting & Idempotency implementation notes

Use Redis for both: rate counters (sliding window or token bucket) and idempotency storage. If Redis unavailable, use in-memory fallback for dev.

Idempotency-Key stored with endpoint + user. When same key + endpoint used again, return stored response snapshot with 409 or 200 depending.

Judge checks mapping

3+ uploads processed -> ensure sample seed includes 5 resumes processed on startup.

/ask returns schema with answer and sources -> provided above.

/match returns evidence and missing reqs -> sample output above.

Pagination -> supported in /api/resumes.

Rate-limit -> 60/min; 429 error format -> implemented.

Minimal FastAPI snippets (conceptual)

Dependencies: fastapi, uvicorn, pydantic, sqlalchemy, python-multipart, pdfplumber, python-docx, faiss-cpu (or use sqlitevector), redis.

main.py (snippets)

from fastapi import FastAPI, UploadFile, File, Header, HTTPException, Depends
from fastapi.security import OAuth2PasswordBearer
import hashlib, uuid, time

app = FastAPI()
oauth2 = OAuth2PasswordBearer(tokenUrl="api/login")

# idempotency middleware sample
idempotency_store = {}  # replace with Redis

def check_idempotency(key: str, user_id: str, endpoint: str):
    k = f"{user_id}:{endpoint}:{key}"
    if k in idempotency_store:
        return idempotency_store[k]
    return None

def set_idempotency(key: str, user_id: str, endpoint: str, value):
    idempotency_store[f"{user_id}:{endpoint}:{key}"] = value

@app.post("/api/resumes")
async def upload_resume(file: UploadFile = File(...), idempotency_key: str = Header(None), token: str = Depends(oauth2)):
    user_id = "u_test"  # decode token
    prev = check_idempotency(idempotency_key, user_id, "/api/resumes")
    if prev:
        return prev
    # save file, parse, chunk, embed
    resource_id = str(uuid.uuid4())
    resp = {"id": resource_id, "status":"processing"}
    set_idempotency(idempotency_key, user_id, "/api/resumes", resp)
    # enqueue processing in background (but for judge make synchronous or processed quickly)
    return resp


Note: For judging, make processing synchronous for first N uploads or provide processed status quickly.

Minimal React UI plan (pages required)

Pages: /upload, /search, /jobs, /candidates/:id.

/upload: file input, show progress, accepts ZIP; sends Idempotency-Key (random UUID).

/search: query input → calls /api/resumes?q= shows results with pagination (next button uses returned next_offset).

/jobs: simple form to create job; list jobs; click to view → GET /api/jobs/:id.

/candidates/:id: shows resume details, redacted text by default and a "Reveal PII (recruiter only)" button.

I can scaffold the React app (Tailwind + fetch) if you want.

Tests & judge checklist

Unit tests:

Upload single resume and check DB row + embeddings created.

Upload ZIP of 3 files and assert 3 resume rows.

GET /api/resumes?q returns matching items & next_offset.

POST /api/ask returns answer + sources with snippets and scores.

POST /api/jobs/:id/match returns evidence & missing requirements.

Rate-limit test: send 61 requests in a minute -> expect 429.

Idempotency test: POST twice with same Idempotency-Key -> single resource returned.

Integration: seed 5 resumes on startup for judge.

Seed data (simple)

Provide 5 small plain-text resumes (I'll list titles + small text you can copy):

r_john_doe.txt

John Doe
Email: john.doe@example.com
Phone: +1-555-123-4567
Experience: 5 years as Backend Engineer. Skills: Python, Django, Flask, PostgreSQL, Kafka, Docker, Kubernetes.
Projects: Real-time ETL pipelines using Kafka and Spark Streaming...


r_jane_smith.txt

Jane Smith
Email: jane.smith@sample.org
Experience: 3 years Data Scientist. Skills: Python, pandas, scikit-learn, TensorFlow, PyTorch. Publications...


r_rahul_sharma.txt

Rahul Sharma
Email: rahul.sharma@mail.in
Experience: 6 years DevOps. Skills: AWS, Terraform, Docker, Kubernetes, Prometheus.


r_maria_garcia.txt

Maria Garcia
Email: maria.garcia@example.es
Frontend: React, Typescript, CSS, HTML, Next.js. Experience: 4 years.


r_alan_kim.txt

Alan Kim
Email: alan.kim@example.co.kr
Fullstack: Node.js, Express, React, MongoDB. Experience: 5 years.

Security & CORS

CORS: open during judging (Access-Control-Allow-Origin: *), but document to restrict in production.

Auth tokens: use JWT with 1-2 hour TTL.

Implementation priorities for hackathon (suggested schedule)

(Hour 0–2) Basic FastAPI app, auth, DB schema, seed résumés.

(2–5) File upload (single & zip), synchronous parse & simple chunking, store parsed_text.

(5–8) Hook embedding (dummy embedding if no model available), FAISS setup, implement /api/resumes, /api/resumes/:id.

(8–11) Implement /api/ask — do vector nearest neighbours + simple answer generation (concat top snippets).

(11–14) Implement jobs endpoints & /match (match by requirement keywords + vector scoring), evidence extraction.

(14–18) Add idempotency & rate limiting (Redis or in-memory), PII redaction, pagination.

(18–24) Build minimal React UI to exercise APIs, write tests, finalize README.
